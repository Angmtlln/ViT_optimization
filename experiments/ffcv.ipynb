{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:11:49.677643Z",
     "start_time": "2025-07-27T14:11:49.673592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from timm import create_model\n",
    "\n",
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import RGBImageField, IntField\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToTensor, ToDevice, ToTorchImage, Cutout\n",
    "from ffcv.fields.decoders import IntDecoder, RandomResizedCropRGBImageDecoder\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) Создаем и сохраняем FFCV датасет (разово)\n",
    "def convert_cifar10_to_ffcv():\n",
    "    train_ds = CIFAR10(root='./data', train=True, download=True)\n",
    "    test_ds = CIFAR10(root='./data', train=False, download=True)\n",
    "\n",
    "    train_writer = DatasetWriter('cifar10_train.beton', {\n",
    "        'image': RGBImageField(max_resolution=256),\n",
    "        'label': IntField()\n",
    "    })\n",
    "    train_writer.from_indexed_dataset(train_ds)\n",
    "\n",
    "    test_writer = DatasetWriter('cifar10_test.beton', {\n",
    "        'image': RGBImageField(max_resolution=256),\n",
    "        'label': IntField()\n",
    "    })\n",
    "    test_writer.from_indexed_dataset(test_ds)\n"
   ],
   "id": "7c67222c689d0246",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:10:18.537312Z",
     "start_time": "2025-07-27T14:10:12.875552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Раскомментируй и запусти один раз\n",
    "convert_cifar10_to_ffcv()"
   ],
   "id": "aed01d414bbd2f6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 99676.18it/s] \n",
      "100%|██████████| 10000/10000 [00:00<00:00, 99753.94it/s]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:14:00.594670Z",
     "start_time": "2025-07-27T14:14:00.380580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ffcv.transforms import NormalizeImage\n",
    "import numpy as np\n",
    "\n",
    "# 2) Настраиваем пайплайны и DataLoader-ы\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "std = np.array([0.2023, 0.1994, 0.2010])\n",
    "\n",
    "train_image_pipeline = [\n",
    "    RandomResizedCropRGBImageDecoder((224, 224)),\n",
    "    Cutout(crop_size=16),                # если хочешь, можно убрать\n",
    "    ToTensor(),\n",
    "    ToTorchImage(),\n",
    "    NormalizeImage(mean, std, np.float32),  # Нормализация + float32\n",
    "    ToDevice(device)\n",
    "]\n",
    "\n",
    "train_label_pipeline = [\n",
    "    IntDecoder(),\n",
    "    ToTensor(),\n",
    "    ToDevice(device)\n",
    "]\n",
    "\n",
    "test_image_pipeline = [\n",
    "    RandomResizedCropRGBImageDecoder((224, 224)),\n",
    "    ToTensor(),\n",
    "    ToTorchImage(),\n",
    "    NormalizeImage(mean, std, np.float32),\n",
    "    ToDevice(device)\n",
    "]\n",
    "test_label_pipeline = [\n",
    "    IntDecoder(),\n",
    "    ToTensor(),\n",
    "    ToDevice(device)\n",
    "]\n",
    "\n",
    "train_loader = Loader('cifar10_train.beton', batch_size=batch_size, num_workers=num_workers,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      pipelines={\n",
    "                          'image': train_image_pipeline,\n",
    "                          'label': train_label_pipeline\n",
    "                      })\n",
    "\n",
    "test_loader = Loader('cifar10_test.beton', batch_size=batch_size, num_workers=num_workers,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     pipelines={\n",
    "                         'image': test_image_pipeline,\n",
    "                         'label': test_label_pipeline\n",
    "                     })"
   ],
   "id": "45763bb291534fb8",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Can't be in JIT mode and on the GPU",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     27\u001B[39m test_image_pipeline = [\n\u001B[32m     28\u001B[39m     RandomResizedCropRGBImageDecoder((\u001B[32m224\u001B[39m, \u001B[32m224\u001B[39m)),\n\u001B[32m     29\u001B[39m     ToTensor(),\n\u001B[32m   (...)\u001B[39m\u001B[32m     32\u001B[39m     ToDevice(device)\n\u001B[32m     33\u001B[39m ]\n\u001B[32m     34\u001B[39m test_label_pipeline = [\n\u001B[32m     35\u001B[39m     IntDecoder(),\n\u001B[32m     36\u001B[39m     ToTensor(),\n\u001B[32m     37\u001B[39m     ToDevice(device)\n\u001B[32m     38\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m train_loader = \u001B[43mLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcifar10_train.beton\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m                      \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mOrderOption\u001B[49m\u001B[43m.\u001B[49m\u001B[43mRANDOM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m                      \u001B[49m\u001B[43mpipelines\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mimage\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_image_pipeline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlabel\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_label_pipeline\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m                      \u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     47\u001B[39m test_loader = Loader(\u001B[33m'\u001B[39m\u001B[33mcifar10_test.beton\u001B[39m\u001B[33m'\u001B[39m, batch_size=batch_size, num_workers=num_workers,\n\u001B[32m     48\u001B[39m                      order=OrderOption.SEQUENTIAL,\n\u001B[32m     49\u001B[39m                      pipelines={\n\u001B[32m     50\u001B[39m                          \u001B[33m'\u001B[39m\u001B[33mimage\u001B[39m\u001B[33m'\u001B[39m: test_image_pipeline,\n\u001B[32m     51\u001B[39m                          \u001B[33m'\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m'\u001B[39m: test_label_pipeline\n\u001B[32m     52\u001B[39m                      })\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/loader/loader.py:210\u001B[39m, in \u001B[36mLoader.__init__\u001B[39m\u001B[34m(self, fname, batch_size, num_workers, os_cache, order, distributed, seed, indices, pipelines, custom_fields, drop_last, batches_ahead, recompile)\u001B[39m\n\u001B[32m    204\u001B[39m         \u001B[38;5;28mself\u001B[39m.pipeline_specs[field_name] = spec\n\u001B[32m    206\u001B[39m \u001B[38;5;28mself\u001B[39m.graph = Graph(\u001B[38;5;28mself\u001B[39m.pipeline_specs, \u001B[38;5;28mself\u001B[39m.reader.handlers,\n\u001B[32m    207\u001B[39m                    \u001B[38;5;28mself\u001B[39m.field_name_to_f_ix, \u001B[38;5;28mself\u001B[39m.reader.metadata,\n\u001B[32m    208\u001B[39m                    memory_read)\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_code\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[38;5;28mself\u001B[39m.first_traversal_order = \u001B[38;5;28mself\u001B[39m.next_traversal_order()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/loader/loader.py:275\u001B[39m, in \u001B[36mLoader.generate_code\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    274\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_code\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m275\u001B[39m     queries, code = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    276\u001B[39m     \u001B[38;5;28mself\u001B[39m.code = \u001B[38;5;28mself\u001B[39m.graph.codegen_all(code)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:352\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    349\u001B[39m     next_nodes = \u001B[38;5;28mself\u001B[39m.adjacency_list[current_node]\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m next_nodes:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallocations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m allocations, code\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:352\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    349\u001B[39m     next_nodes = \u001B[38;5;28mself\u001B[39m.adjacency_list[current_node]\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m next_nodes:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallocations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m allocations, code\n",
      "    \u001B[31m[... skipping similar frames: Graph.collect_requirements at line 352 (3 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:352\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    349\u001B[39m     next_nodes = \u001B[38;5;28mself\u001B[39m.adjacency_list[current_node]\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m next_nodes:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallocations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m allocations, code\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:330\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    327\u001B[39m operation.accept_field(\u001B[38;5;28mself\u001B[39m.handlers[source_field])\n\u001B[32m    328\u001B[39m operation.accept_globals(metadata, \u001B[38;5;28mself\u001B[39m.memory_read)\n\u001B[32m--> \u001B[39m\u001B[32m330\u001B[39m next_state, allocation = \u001B[43moperation\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdeclare_state_and_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    331\u001B[39m state_allocation = operation.declare_shared_memory(state)\n\u001B[32m    333\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m next_state.device.type != \u001B[33m'\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(operation,\n\u001B[32m    334\u001B[39m     ModuleWrapper):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/transforms/ops.py:62\u001B[39m, in \u001B[36mToDevice.declare_state_and_memory\u001B[39m\u001B[34m(self, previous_state)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdeclare_state_and_memory\u001B[39m(\u001B[38;5;28mself\u001B[39m, previous_state: State) -> Tuple[State, Optional[AllocationQuery]]:\n\u001B[32m---> \u001B[39m\u001B[32m62\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprevious_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, AllocationQuery(previous_state.shape, dtype=previous_state.dtype, device=\u001B[38;5;28mself\u001B[39m.device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/dataclasses.py:1581\u001B[39m, in \u001B[36mreplace\u001B[39m\u001B[34m(obj, **changes)\u001B[39m\n\u001B[32m   1574\u001B[39m         changes[f.name] = \u001B[38;5;28mgetattr\u001B[39m(obj, f.name)\n\u001B[32m   1576\u001B[39m \u001B[38;5;66;03m# Create the new object, which calls __init__() and\u001B[39;00m\n\u001B[32m   1577\u001B[39m \u001B[38;5;66;03m# __post_init__() (if defined), using all of the init fields we've\u001B[39;00m\n\u001B[32m   1578\u001B[39m \u001B[38;5;66;03m# added and/or left in 'changes'.  If there are values supplied in\u001B[39;00m\n\u001B[32m   1579\u001B[39m \u001B[38;5;66;03m# changes that aren't fields, this will correctly raise a\u001B[39;00m\n\u001B[32m   1580\u001B[39m \u001B[38;5;66;03m# TypeError.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1581\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__class__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mchanges\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:7\u001B[39m, in \u001B[36m__init__\u001B[39m\u001B[34m(self, jit_mode, device, shape, dtype)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/state.py:18\u001B[39m, in \u001B[36mState.__post_init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__post_init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     17\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.jit_mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device != ch.device(\u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mCan\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt be in JIT mode and on the GPU\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.jit_mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.dtype, ch.dtype):\n\u001B[32m     20\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mCan\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt allocate a torch tensor in JIT mode\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAssertionError\u001B[39m: Can't be in JIT mode and on the GPU"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:12:28.352138Z",
     "start_time": "2025-07-27T14:12:26.409308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3) Создаем модель DeiT (с timm)\n",
    "model = create_model('deit_base_distilled_patch16_224', pretrained=True, num_classes=10)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# 4) Обучение и валидация\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} train\"):\n",
    "        images, labels = batch[0], batch[1].long().squeeze()\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (preds.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    print(f\"Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} val\"):\n",
    "            images, labels = batch['image'], batch['label'].long().squeeze()\n",
    "            preds = model(images)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (preds.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    test_acc = 100 * test_correct / test_total\n",
    "    print(f\"Validation accuracy: {test_acc:.2f}%\")"
   ],
   "id": "8ebae587e9be0243",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 train:   0%|          | 0/390 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[35]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m train\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m     17\u001B[39m     images, labels = batch[\u001B[32m0\u001B[39m], batch[\u001B[32m1\u001B[39m].long().squeeze()\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     preds = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m     loss = criterion(preds, labels)\n\u001B[32m     21\u001B[39m     optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/timm/models/vision_transformer.py:993\u001B[39m, in \u001B[36mVisionTransformer.forward\u001B[39m\u001B[34m(self, x, attn_mask)\u001B[39m\n\u001B[32m    992\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m) -> torch.Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m993\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    994\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.forward_head(x)\n\u001B[32m    995\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/timm/models/vision_transformer.py:936\u001B[39m, in \u001B[36mVisionTransformer.forward_features\u001B[39m\u001B[34m(self, x, attn_mask)\u001B[39m\n\u001B[32m    934\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward_features\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m) -> torch.Tensor:\n\u001B[32m    935\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Forward pass through feature layers (embeddings, transformer blocks, post-transformer norm).\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m936\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpatch_embed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    937\u001B[39m     x = \u001B[38;5;28mself\u001B[39m._pos_embed(x)\n\u001B[32m    938\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.patch_drop(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/timm/layers/patch_embed.py:131\u001B[39m, in \u001B[36mPatchEmbed.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    129\u001B[39m     pad_w = (\u001B[38;5;28mself\u001B[39m.patch_size[\u001B[32m1\u001B[39m] - W % \u001B[38;5;28mself\u001B[39m.patch_size[\u001B[32m1\u001B[39m]) % \u001B[38;5;28mself\u001B[39m.patch_size[\u001B[32m1\u001B[39m]\n\u001B[32m    130\u001B[39m     x = F.pad(x, (\u001B[32m0\u001B[39m, pad_w, \u001B[32m0\u001B[39m, pad_h))\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mproj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.flatten:\n\u001B[32m    133\u001B[39m     x = x.flatten(\u001B[32m2\u001B[39m).transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)  \u001B[38;5;66;03m# NCHW -> NLC\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f7e37d43472e23c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:49:13.845311Z",
     "start_time": "2025-07-27T13:49:05.164950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True)\n",
    "\n",
    "writer = DatasetWriter('cifar_train.beton', {\n",
    "    'image': RGBImageField(write_mode='jpg'),\n",
    "    'label': IntField()\n",
    "})\n",
    "writer.from_indexed_dataset(train_dataset)\n",
    "\n",
    "writer = DatasetWriter('cifar_test.beton', {\n",
    "    'image': RGBImageField(write_mode='jpg'),\n",
    "    'label': IntField()\n",
    "})\n",
    "writer.from_indexed_dataset(test_dataset)\n"
   ],
   "id": "f8d82609d626cca1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 159438.38it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 99691.35it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:06:48.627663Z",
     "start_time": "2025-07-27T14:06:48.624311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToDevice, ToTensor, NormalizeImage, RandomHorizontalFlip, RandomResizedCrop\n",
    "from ffcv.transforms.common import Squeeze\n",
    "from timm import create_model\n",
    "from tqdm import tqdm\n",
    "from ffcv.transforms import RandomResizedCrop\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "1a3b62bcf7f50dd2",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:06:55.059244Z",
     "start_time": "2025-07-27T14:06:54.451513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Параметры ===\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 20\n",
    "LR = 3e-4\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mean = np.array([0.4914 * 255, 0.4822 * 255, 0.4465 * 255], dtype=np.float32)\n",
    "std = np.array([0.2023 * 255, 0.1994 * 255, 0.2010 * 255], dtype=np.float32)\n",
    "\n",
    "\n",
    "train_transforms = [\n",
    "    RandomResizedCrop(224, (0.8, 1.0), (0.75, 1.333)),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    ToDevice(device),\n",
    "    NormalizeImage(mean, std, type=np.float32)\n",
    "]\n",
    "\n",
    "label_pipeline = [Squeeze(), ToTensor(), ToDevice(device)]\n",
    "\n",
    "train_loader = Loader('cifar_train.beton', batch_size=BATCH_SIZE, num_workers=4,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      pipelines={\n",
    "                          'image': train_transforms,\n",
    "                          'label': label_pipeline\n",
    "                      })\n",
    "\n",
    "test_loader = Loader('cifar_test.beton', batch_size=BATCH_SIZE, num_workers=2,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     pipelines={\n",
    "                         'image': [\n",
    "                             RandomResizedCrop((224, 224), scale=(1.0, 1.0)),  # resize only\n",
    "                             ToTensor(),\n",
    "                             ToDevice(device),\n",
    "                             NormalizeImage(mean, std, type=np.float32)\n",
    "\n",
    "                         ],\n",
    "                         'label': label_pipeline\n",
    "                     })\n"
   ],
   "id": "47851ac79ef44a59",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     13\u001B[39m train_transforms = [\n\u001B[32m     14\u001B[39m     RandomResizedCrop(\u001B[32m224\u001B[39m, (\u001B[32m0.8\u001B[39m, \u001B[32m1.0\u001B[39m), (\u001B[32m0.75\u001B[39m, \u001B[32m1.333\u001B[39m)),\n\u001B[32m     15\u001B[39m     RandomHorizontalFlip(),\n\u001B[32m   (...)\u001B[39m\u001B[32m     18\u001B[39m     NormalizeImage(mean, std, \u001B[38;5;28mtype\u001B[39m=np.float32)\n\u001B[32m     19\u001B[39m ]\n\u001B[32m     21\u001B[39m label_pipeline = [Squeeze(), ToTensor(), ToDevice(device)]\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m train_loader = \u001B[43mLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcifar_train.beton\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m                      \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mOrderOption\u001B[49m\u001B[43m.\u001B[49m\u001B[43mRANDOM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m                      \u001B[49m\u001B[43mpipelines\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mimage\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_transforms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlabel\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_pipeline\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m                      \u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m test_loader = Loader(\u001B[33m'\u001B[39m\u001B[33mcifar_test.beton\u001B[39m\u001B[33m'\u001B[39m, batch_size=BATCH_SIZE, num_workers=\u001B[32m2\u001B[39m,\n\u001B[32m     31\u001B[39m                      order=OrderOption.SEQUENTIAL,\n\u001B[32m     32\u001B[39m                      pipelines={\n\u001B[32m   (...)\u001B[39m\u001B[32m     40\u001B[39m                          \u001B[33m'\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m'\u001B[39m: label_pipeline\n\u001B[32m     41\u001B[39m                      })\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/loader/loader.py:210\u001B[39m, in \u001B[36mLoader.__init__\u001B[39m\u001B[34m(self, fname, batch_size, num_workers, os_cache, order, distributed, seed, indices, pipelines, custom_fields, drop_last, batches_ahead, recompile)\u001B[39m\n\u001B[32m    204\u001B[39m         \u001B[38;5;28mself\u001B[39m.pipeline_specs[field_name] = spec\n\u001B[32m    206\u001B[39m \u001B[38;5;28mself\u001B[39m.graph = Graph(\u001B[38;5;28mself\u001B[39m.pipeline_specs, \u001B[38;5;28mself\u001B[39m.reader.handlers,\n\u001B[32m    207\u001B[39m                    \u001B[38;5;28mself\u001B[39m.field_name_to_f_ix, \u001B[38;5;28mself\u001B[39m.reader.metadata,\n\u001B[32m    208\u001B[39m                    memory_read)\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_code\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[38;5;28mself\u001B[39m.first_traversal_order = \u001B[38;5;28mself\u001B[39m.next_traversal_order()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/loader/loader.py:275\u001B[39m, in \u001B[36mLoader.generate_code\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    274\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_code\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m275\u001B[39m     queries, code = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    276\u001B[39m     \u001B[38;5;28mself\u001B[39m.code = \u001B[38;5;28mself\u001B[39m.graph.codegen_all(code)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:352\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    349\u001B[39m     next_nodes = \u001B[38;5;28mself\u001B[39m.adjacency_list[current_node]\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m next_nodes:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallocations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m allocations, code\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:352\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    349\u001B[39m     next_nodes = \u001B[38;5;28mself\u001B[39m.adjacency_list[current_node]\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m next_nodes:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallocations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m allocations, code\n",
      "    \u001B[31m[... skipping similar frames: Graph.collect_requirements at line 352 (3 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:352\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    349\u001B[39m     next_nodes = \u001B[38;5;28mself\u001B[39m.adjacency_list[current_node]\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m next_nodes:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollect_requirements\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallocations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource_field\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m allocations, code\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:346\u001B[39m, in \u001B[36mGraph.collect_requirements\u001B[39m\u001B[34m(self, state, current_node, allocations, code, source_field)\u001B[39m\n\u001B[32m    344\u001B[39m     allocations[\u001B[33m'\u001B[39m\u001B[33moperation\u001B[39m\u001B[33m'\u001B[39m][current_node.id] = allocation\n\u001B[32m    345\u001B[39m     allocations[\u001B[33m'\u001B[39m\u001B[33mshared\u001B[39m\u001B[33m'\u001B[39m][current_node.id] = state_allocation\n\u001B[32m--> \u001B[39m\u001B[32m346\u001B[39m     code[\u001B[33m'\u001B[39m\u001B[33moperation\u001B[39m\u001B[33m'\u001B[39m][current_node.id] = \u001B[43moperation\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_code\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    347\u001B[39m     code[\u001B[33m'\u001B[39m\u001B[33mshared\u001B[39m\u001B[33m'\u001B[39m][current_node.id] = operation.generate_code_for_shared_state()\n\u001B[32m    349\u001B[39m next_nodes = \u001B[38;5;28mself\u001B[39m.adjacency_list[current_node]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/transforms/normalize.py:56\u001B[39m, in \u001B[36mNormalizeImage.generate_code\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.mode == \u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m     55\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.generate_code_cpu()\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_code_gpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/transforms/normalize.py:61\u001B[39m, in \u001B[36mNormalizeImage.generate_code_gpu\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_code_gpu\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Callable:\n\u001B[32m     59\u001B[39m \n\u001B[32m     60\u001B[39m     \u001B[38;5;66;03m# We only import cupy if it's truly needed\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcupy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcp\u001B[39;00m\n\u001B[32m     62\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpytorch_pfn_extras\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mppe\u001B[39;00m\n\u001B[32m     64\u001B[39m     tn = np.zeros((), dtype=\u001B[38;5;28mself\u001B[39m.dtype).dtype.name\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'cupy'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# === Модель DeiT ===\n",
    "model = create_model('deit_base_distilled_patch16_224', pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# === Оптимизатор и лосс ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# === Тренировка ===\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images, labels = batch['image'], batch['label'].long().squeeze()\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (preds.argmax(1) == labels).sum().item()\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    print(f\"Train Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    # === Валидация ===\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch['image'], batch['label'].long().squeeze()\n",
    "            preds = model(images)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds.argmax(1) == labels).sum().item()\n",
    "    val_acc = correct / total * 100\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n"
   ],
   "id": "74d8a8ac7ef128e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:45:00.240139Z",
     "start_time": "2025-07-27T13:44:59.735490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "beton_path = './data/cifar10_train.beton'\n",
    "\n",
    "\n",
    "train_loader = Loader(\n",
    "    beton_path,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    order=OrderOption.RANDOM,\n",
    "    drop_last=True,\n",
    "    pipelines={\n",
    "        'image': [\n",
    "            SimpleRGBImageDecoder(),\n",
    "            ToTensor(),\n",
    "            ToDevice(device),\n",
    "            ToTorchImage(),\n",
    "            Convert(torch.float32),\n",
    "        ],\n",
    "        'label': [\n",
    "            IntDecoder(),\n",
    "            ToTensor(),\n",
    "            ToDevice(device),\n",
    "            Convert(torch.long)\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "id": "cf958643e14ad856",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "876d7c6e853d737f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:45:08.678548Z",
     "start_time": "2025-07-27T13:45:06.557748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "from transformers import DeiTForImageClassification, DeiTFeatureExtractor\n",
    "\n",
    "\n",
    "model = timm.create_model('deit_base_distilled_patch16_224', pretrained=False, num_classes=10)\n",
    "model = model.to(device)"
   ],
   "id": "ea6e9603ea781a61",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:46:58.539593Z",
     "start_time": "2025-07-27T13:45:20.036942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToTensor, ToDevice, ToTorchImage, Cutout, NormalizeImage\n",
    "from ffcv.fields.decoders import IntDecoder, RandomResizedCropRGBImageDecoder\n",
    "import numpy as np\n",
    "\n",
    "image_pipeline = [\n",
    "    RandomResizedCropRGBImageDecoder((224, 224)),\n",
    "    ToTensor(),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float32),\n",
    "    NormalizeImage(\n",
    "        mean=np.array([0.485 * 255, 0.456 * 255, 0.406 * 255], dtype=np.float32),\n",
    "        std=np.array([0.229 * 255, 0.224 * 255, 0.225 * 255], dtype=np.float32),\n",
    "        type=np.float32\n",
    "    )\n",
    "]\n",
    "\n",
    "label_pipeline = [\n",
    "    IntDecoder(),\n",
    "    ToTensor(),\n",
    "    Convert(torch.long)\n",
    "]\n",
    "\n",
    "train_loader = Loader(\n",
    "    './data/cifar10_train.beton',\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    order=OrderOption.RANDOM,\n",
    "    drop_last=True,\n",
    "    pipelines={\n",
    "        'image': image_pipeline,\n",
    "        'label': label_pipeline\n",
    "    },\n",
    "    os_cache=False,      # отключить JIT\n",
    "    recompile=False      # тоже отключить повторную компиляцию (JIT)\n",
    ")"
   ],
   "id": "9283dd7a554f6b26",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m      6\u001B[39m image_pipeline = [\n\u001B[32m      7\u001B[39m     RandomResizedCropRGBImageDecoder((\u001B[32m224\u001B[39m, \u001B[32m224\u001B[39m)),\n\u001B[32m      8\u001B[39m     ToTensor(),\n\u001B[32m   (...)\u001B[39m\u001B[32m     15\u001B[39m     )\n\u001B[32m     16\u001B[39m ]\n\u001B[32m     18\u001B[39m label_pipeline = [\n\u001B[32m     19\u001B[39m     IntDecoder(),\n\u001B[32m     20\u001B[39m     ToTensor(),\n\u001B[32m     21\u001B[39m     Convert(torch.long)\n\u001B[32m     22\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m train_loader = \u001B[43mLoader\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m./data/cifar10_train.beton\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mOrderOption\u001B[49m\u001B[43m.\u001B[49m\u001B[43mRANDOM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdrop_last\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpipelines\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mimage\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_pipeline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlabel\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_pipeline\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43mos_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# отключить JIT\u001B[39;49;00m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrecompile\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# тоже отключить повторную компиляцию (JIT)\u001B[39;49;00m\n\u001B[32m     36\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/loader/loader.py:210\u001B[39m, in \u001B[36mLoader.__init__\u001B[39m\u001B[34m(self, fname, batch_size, num_workers, os_cache, order, distributed, seed, indices, pipelines, custom_fields, drop_last, batches_ahead, recompile)\u001B[39m\n\u001B[32m    204\u001B[39m         \u001B[38;5;28mself\u001B[39m.pipeline_specs[field_name] = spec\n\u001B[32m    206\u001B[39m \u001B[38;5;28mself\u001B[39m.graph = Graph(\u001B[38;5;28mself\u001B[39m.pipeline_specs, \u001B[38;5;28mself\u001B[39m.reader.handlers,\n\u001B[32m    207\u001B[39m                    \u001B[38;5;28mself\u001B[39m.field_name_to_f_ix, \u001B[38;5;28mself\u001B[39m.reader.metadata,\n\u001B[32m    208\u001B[39m                    memory_read)\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_code\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[38;5;28mself\u001B[39m.first_traversal_order = \u001B[38;5;28mself\u001B[39m.next_traversal_order()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/loader/loader.py:276\u001B[39m, in \u001B[36mLoader.generate_code\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    274\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_code\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    275\u001B[39m     queries, code = \u001B[38;5;28mself\u001B[39m.graph.collect_requirements()\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m     \u001B[38;5;28mself\u001B[39m.code = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcodegen_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:476\u001B[39m, in \u001B[36mGraph.codegen_all\u001B[39m\u001B[34m(self, code)\u001B[39m\n\u001B[32m    475\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcodegen_all\u001B[39m(\u001B[38;5;28mself\u001B[39m, code):\n\u001B[32m--> \u001B[39m\u001B[32m476\u001B[39m     stages = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroup_operations\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    477\u001B[39m     code_stages = []\n\u001B[32m    478\u001B[39m     already_defined = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/ffcv/pipeline/graph.py:398\u001B[39m, in \u001B[36mGraph.group_operations\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    395\u001B[39m         current_front.update(\u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m.adjacency_list[node]))\n\u001B[32m    397\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m         \u001B[43mnext_front\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    400\u001B[39m stages.append(current_stage)\n\u001B[32m    401\u001B[39m current_front = next_front\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T11:55:18.152578Z",
     "start_time": "2025-07-22T11:55:18.020542Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "372058830c35e260"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T11:55:18.381054Z",
     "start_time": "2025-07-22T11:55:18.377912Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 335218 KiB | 335218 KiB | 335218 KiB |      0 B   |\n",
      "|       from large pool | 334080 KiB | 334080 KiB | 334080 KiB |      0 B   |\n",
      "|       from small pool |   1138 KiB |   1138 KiB |   1138 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 335218 KiB | 335218 KiB | 335218 KiB |      0 B   |\n",
      "|       from large pool | 334080 KiB | 334080 KiB | 334080 KiB |      0 B   |\n",
      "|       from small pool |   1138 KiB |   1138 KiB |   1138 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 335217 KiB | 335217 KiB | 335217 KiB |      0 B   |\n",
      "|       from large pool | 334080 KiB | 334080 KiB | 334080 KiB |      0 B   |\n",
      "|       from small pool |   1137 KiB |   1137 KiB |   1137 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |\n",
      "|       from large pool | 389120 KiB | 389120 KiB | 389120 KiB |      0 B   |\n",
      "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  55950 KiB |  56620 KiB | 264445 KiB | 208495 KiB |\n",
      "|       from large pool |  55040 KiB |  55040 KiB | 262400 KiB | 207360 KiB |\n",
      "|       from small pool |    910 KiB |   2045 KiB |   2045 KiB |   1135 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     155    |     155    |     155    |       0    |\n",
      "|       from large pool |      49    |      49    |      49    |       0    |\n",
      "|       from small pool |     106    |     106    |     106    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     155    |     155    |     155    |       0    |\n",
      "|       from large pool |      49    |      49    |      49    |       0    |\n",
      "|       from small pool |     106    |     106    |     106    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
      "|       from large pool |      19    |      19    |      19    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      20    |      20    |      20    |       0    |\n",
      "|       from large pool |      19    |      19    |      19    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": "print(torch.cuda.memory_summary(device=None, abbreviated=False))",
   "id": "ed6e419611addc4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T12:06:06.848044Z",
     "start_time": "2025-07-22T11:55:24.639834Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43201/481034838.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1:   0%|          | 0/3125 [00:00<?, ?it/s]/tmp/ipykernel_43201/481034838.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|██████████| 3125/3125 [03:35<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Loss: 6199.151, Accuracy: 25.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3125/3125 [03:33<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Loss: 5571.412, Accuracy: 33.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3125/3125 [03:33<00:00, 14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Loss: 5231.953, Accuracy: 38.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images, labels = images.to(device), labels.squeeze(1).to(device)  # squeeze для labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(pixel_values=images).logits\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, pred = output.max(1)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1} — Loss: {total_loss:.3f}, Accuracy: {acc:.2f}%\")\n"
   ],
   "id": "40d0f10d8aab3e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80eeccbba7ede2a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
