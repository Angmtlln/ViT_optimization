{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e455471",
   "metadata": {},
   "source": [
    "# 1️⃣ IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ea51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU is available. Using CUDA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, random, time\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('✓ GPU is available. Using CUDA')\n",
    "else:\n",
    "    print('⚠ Using CPU')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa76795",
   "metadata": {},
   "source": [
    "# 2️⃣ MODEL DOWNLOAD / SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3a4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Конфиг\n",
    "ROOT_DIR = 'kvasir-dataset-v2'\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0  # Windows-safe\n",
    "\n",
    "# Teacher probe\n",
    "TEACHER_LR = 5e-4\n",
    "TEACHER_EPOCHS = 5\n",
    "\n",
    "# Attention distillation\n",
    "AT_LR = 1e-4\n",
    "AT_EPOCHS = 15\n",
    "AT_ALPHA = 0.6  # вес attention MSE, (1-AT_ALPHA) — CE\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "SAVE_BEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f256a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\Рабочий стол\\ViT_opti\\ViT_optimization\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12aae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\Рабочий стол\\ViT_opti\\ViT_optimization\\perception_models\n"
     ]
    }
   ],
   "source": [
    "cd perception_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe89c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\Рабочий стол\\ViT_opti\\ViT_optimization\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем модель-учителя PE-Core-L14-336...\n",
      "Missing keys for loading model: []\n",
      "Unexpected keys for loading model: []\n",
      "Загружаем модель-студента PE-Core-T16-384...\n",
      "Missing keys for loading model: []\n",
      "Unexpected keys for loading model: []\n",
      "Размеры: teacher_img=336, student_img=384\n",
      "Dims: teacher=1024, student=512\n"
     ]
    }
   ],
   "source": [
    "from core.vision_encoder import pe\n",
    "from core.vision_encoder import transforms\n",
    "\n",
    "print('Загружаем модель-учителя PE-Core-L14-336...')\n",
    "teacher_model = pe.CLIP.from_config('PE-Core-L14-336', pretrained=True).to(device).float().eval()\n",
    "\n",
    "print('Загружаем модель-студента PE-Core-T16-384...')\n",
    "student_model = pe.CLIP.from_config('PE-Core-T16-384', pretrained=True).to(device).float()\n",
    "\n",
    "teacher_preprocessor = transforms.get_image_transform(teacher_model.image_size)\n",
    "student_preprocessor = transforms.get_image_transform(student_model.image_size)\n",
    "teacher_dim = teacher_model.visual.output_dim\n",
    "student_dim = student_model.visual.output_dim\n",
    "\n",
    "print(f'Размеры: teacher_img={teacher_model.image_size}, student_img={student_model.image_size}')\n",
    "print(f'Dims: teacher={teacher_dim}, student={student_dim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ee581",
   "metadata": {},
   "source": [
    "# 3️⃣ DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8d7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\Рабочий стол\\ViT_opti\\ViT_optimization\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ecfb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\Рабочий стол\\ViT_opti\\ViT_optimization\\data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4b361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ищем изображения...\n",
      "Найдено 8000 jpg-файлов\n",
      "Валидных изображений: 7999\n",
      "Классов: 8 -> ['dyed-lifted-polyps', 'dyed-resection-margins', 'esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis']\n",
      "Подмножество: 4000 изображений\n",
      "Train: 2560  Val: 640  Test: 800\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Подготовка датасета\n",
    "# ==============================\n",
    "def extract_label_from_path(image_path, root_dir):\n",
    "    parts = Path(image_path).parts\n",
    "    try:\n",
    "        ridx = parts.index(Path(root_dir).name)\n",
    "        return parts[ridx + 1]\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "print('Ищем изображения...')\n",
    "all_image_paths = glob(os.path.join(ROOT_DIR, '**', '*.jpg'), recursive=True)\n",
    "print(f'Найдено {len(all_image_paths)} jpg-файлов')\n",
    "\n",
    "image_to_path, image_to_label = {}, {}\n",
    "for p in all_image_paths:\n",
    "    image_id = os.path.splitext(os.path.basename(p))[0]\n",
    "    label = extract_label_from_path(p, ROOT_DIR)\n",
    "    if label is not None:\n",
    "        image_to_path[image_id] = p\n",
    "        image_to_label[image_id] = label\n",
    "print(f'Валидных изображений: {len(image_to_label)}')\n",
    "\n",
    "classes = sorted(list(set(image_to_label.values())))\n",
    "label_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "idx_to_label = {i: c for c, i in label_to_idx.items()}\n",
    "print(f'Классов: {len(classes)} -> {classes}')\n",
    "\n",
    "# Сбалансированное подмножество\n",
    "images_by_class = defaultdict(list)\n",
    "for img_id, lbl in image_to_label.items():\n",
    "    images_by_class[lbl].append(img_id)\n",
    "\n",
    "images_per_class = 500\n",
    "sampled = []\n",
    "for lbl, ids in images_by_class.items():\n",
    "    k = min(images_per_class, len(ids))\n",
    "    sampled.extend(random.sample(ids, k))\n",
    "random.shuffle(sampled)\n",
    "print(f'Подмножество: {len(sampled)} изображений')\n",
    "\n",
    "labels_for_strat = [image_to_label[i] for i in sampled]\n",
    "train_val_ids, test_ids = train_test_split(sampled, test_size=0.20, stratify=labels_for_strat, random_state=SEED)\n",
    "train_labels_for_strat = [image_to_label[i] for i in train_val_ids]\n",
    "train_ids, val_ids = train_test_split(train_val_ids, test_size=0.20, stratify=train_labels_for_strat, random_state=SEED)\n",
    "\n",
    "print(f'Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88703578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataLoaders готовы\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Dataset и DataLoader\n",
    "# ==============================\n",
    "class AttnDataset(Dataset):\n",
    "    def __init__(self, image_ids, image_to_path, image_to_label, label_to_idx, t_transform, s_transform):\n",
    "        self.ids = image_ids\n",
    "        self.image_to_path = image_to_path\n",
    "        self.image_to_label = image_to_label\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.t_transform = t_transform\n",
    "        self.s_transform = s_transform\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        path = self.image_to_path[img_id]\n",
    "        label = self.image_to_label[img_id]\n",
    "        y = self.label_to_idx[label]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        x_t = self.t_transform(img)\n",
    "        x_s = self.s_transform(img)\n",
    "        return x_t, x_s, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_ds = AttnDataset(train_ids, image_to_path, image_to_label, label_to_idx, teacher_preprocessor, student_preprocessor)\n",
    "val_ds   = AttnDataset(val_ids,   image_to_path, image_to_label, label_to_idx, teacher_preprocessor, student_preprocessor)\n",
    "test_ds  = AttnDataset(test_ids,  image_to_path, image_to_label, label_to_idx, teacher_preprocessor, student_preprocessor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "print('✓ DataLoaders готовы')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34577734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инициализированы головы: teacher_head 1024->8, student_head 512->8\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Головы классификации\n",
    "# ==============================\n",
    "class LinearHead(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "num_classes = len(classes)\n",
    "teacher_head = LinearHead(teacher_dim, num_classes).to(device)\n",
    "student_head = LinearHead(student_dim, num_classes).to(device)\n",
    "print(f'Инициализированы головы: teacher_head {teacher_dim}->{num_classes}, student_head {student_dim}->{num_classes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743dff9",
   "metadata": {},
   "source": [
    "# 4️⃣ MODEL PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher Probe] Epoch 1/5 | TrainAcc 68.91% | ValAcc 82.19% | ValLoss 0.6337\n",
      "[Teacher Probe] Epoch 2/5 | TrainAcc 85.43% | ValAcc 88.28% | ValLoss 0.4447\n",
      "[Teacher Probe] Epoch 3/5 | TrainAcc 87.30% | ValAcc 87.66% | ValLoss 0.3766\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Обучение teacher linear probe (замороженный учитель)\n",
    "# ==============================\n",
    "def evaluate_head(model, head, loader, device):\n",
    "    model.eval(); head.eval()\n",
    "    correct, total, loss_sum, n_batches = 0, 0, 0.0, 0\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x_t, _, y in loader:\n",
    "            x_t = x_t.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "            feats = model.encode_image(x_t)\n",
    "            logits = head(feats)\n",
    "            loss = ce(logits, y)\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            loss_sum += loss.item()\n",
    "            n_batches += 1\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return (loss_sum / max(1, n_batches)), acc\n",
    "\n",
    "def train_teacher_probe(model, head, train_loader, val_loader, epochs=TEACHER_EPOCHS, lr=TEACHER_LR):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    opt = torch.optim.AdamW(head.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    best_val_acc, best_state = -1.0, None\n",
    "    for ep in range(1, epochs + 1):\n",
    "        head.train()\n",
    "        loss_sum, n_batches, correct, total = 0.0, 0, 0, 0\n",
    "        for x_t, _, y in train_loader:\n",
    "            x_t = x_t.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "            with torch.no_grad():\n",
    "                feats = model.encode_image(x_t)\n",
    "            logits = head(feats)\n",
    "            loss = ce(logits, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            loss_sum += loss.item(); n_batches += 1\n",
    "        train_acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "        val_loss, val_acc = evaluate_head(model, head, val_loader, device)\n",
    "        print(f'[Teacher Probe] Epoch {ep}/{epochs} | TrainAcc {train_acc:.2f}% | ValAcc {val_acc:.2f}% | ValLoss {val_loss:.4f}')\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = head.state_dict()\n",
    "    if best_state is not None:\n",
    "        head.load_state_dict(best_state)\n",
    "    for p in head.parameters():\n",
    "        p.requires_grad = False\n",
    "    head.eval()\n",
    "    torch.save({'state_dict': head.state_dict(), 'num_classes': num_classes}, 'teacher_linear_probe_attn.pth')\n",
    "    print(f'✓ Сохранен teacher linear probe (ValAcc={best_val_acc:.2f}%)')\n",
    "\n",
    "train_teacher_probe(teacher_model, teacher_head, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Зарегистрированы forward-хуки на visual.transformer (teacher/student)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Хуки для токенов трансформера (ViT)\n",
    "# ==============================\n",
    "class ModuleOutputHook:\n",
    "    def __init__(self, module):\n",
    "        self.outputs = None\n",
    "        self.handle = module.register_forward_hook(self._hook)\n",
    "    def _hook(self, module, inputs, output):\n",
    "        # сохраняем выход (обычно [B, N, D] для transformer)\n",
    "        self.outputs = output\n",
    "    def close(self):\n",
    "        if self.handle is not None:\n",
    "            self.handle.remove(); self.handle = None\n",
    "\n",
    "def get_transformer_module(clip_model):\n",
    "    # Пытаемся получить модуль трансформера для ViT\n",
    "    visual = getattr(clip_model, 'visual', None)\n",
    "    assert visual is not None, 'visual не найден у CLIP-модели'\n",
    "    transformer = getattr(visual, 'transformer', None)\n",
    "    assert transformer is not None, 'visual.transformer не найден (ожидается ViT)'\n",
    "    return transformer\n",
    "\n",
    "teacher_transformer = get_transformer_module(teacher_model)\n",
    "student_transformer = get_transformer_module(student_model)\n",
    "teacher_hook = ModuleOutputHook(teacher_transformer)\n",
    "student_hook = ModuleOutputHook(student_transformer)\n",
    "print('✓ Зарегистрированы forward-хуки на visual.transformer (teacher/student)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b62eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Функции внимания готовы\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Построение карт внимания из токенов\n",
    "# ==============================\n",
    "def tokens_to_attention_map(tokens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    tokens: [B, N, D] или [B, N+1, D] (если есть CLS токен).\n",
    "    Возвращает нормированную карту внимания [B, H, W] как сумму квадратов по каналам.\n",
    "    \"\"\"\n",
    "    assert tokens.dim() == 3, 'Ожидается тензор с формой [B, N, D]'\n",
    "    B, N, D = tokens.shape\n",
    "    s = int(math.sqrt(N))\n",
    "    if s * s == N:\n",
    "        patch_tokens = tokens  # CLS нет\n",
    "        H = W = s\n",
    "    else:\n",
    "        # предполагаем, что первый токен — CLS, значит патчей N-1 = H*W\n",
    "        Np = N - 1\n",
    "        H = W = int(math.sqrt(Np))\n",
    "        assert H * W == Np, 'Число патч-токенов не квадрат: не удается построить карту'\n",
    "        patch_tokens = tokens[:, 1:, :]\n",
    "    fmap = patch_tokens.view(B, H, W, D)\n",
    "    attn = (fmap.pow(2).sum(dim=-1))  # [B, H, W]\n",
    "    # L2-нормализация по карте, чтобы сравнение было масштаб-инвариантным\n",
    "    attn = attn / (attn.norm(p=2, dim=(1, 2), keepdim=True) + 1e-6)\n",
    "    return attn\n",
    "\n",
    "def match_attention_maps(attn_s: torch.Tensor, attn_t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Приводит карты к одному разрешению и считает MSE.\n",
    "    attn_*: [B, H, W]\n",
    "    \"\"\"\n",
    "    if attn_s.shape != attn_t.shape:\n",
    "        attn_t_ = F.interpolate(attn_t.unsqueeze(1), size=attn_s.shape[-2:], mode='bilinear', align_corners=False).squeeze(1)\n",
    "    else:\n",
    "        attn_t_ = attn_t\n",
    "    return F.mse_loss(attn_s, attn_t_)\n",
    "\n",
    "print('✓ Функции внимания готовы')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3478aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Инициализирован критерий AttentionDistillLoss\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Критерий: Attention MSE + CE\n",
    "# ==============================\n",
    "class AttentionDistillLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.6):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "    def forward(self, s_logits, y, attn_s, attn_t):\n",
    "        ce_loss = self.ce(s_logits, y)\n",
    "        attn_loss = match_attention_maps(attn_s, attn_t)\n",
    "        total = self.alpha * attn_loss + (1.0 - self.alpha) * ce_loss\n",
    "        return total, attn_loss, ce_loss\n",
    "\n",
    "criterion = AttentionDistillLoss(alpha=AT_ALPHA)\n",
    "print('✓ Инициализирован критерий AttentionDistillLoss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b95308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Функция валидации готова\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Валидация\n",
    "# ==============================\n",
    "def validate(student_model, student_head, teacher_model, teacher_head, loader, device):\n",
    "    student_model.eval(); student_head.eval()\n",
    "    teacher_model.eval(); teacher_head.eval()\n",
    "    total_loss = total_attn = total_ce = 0.0\n",
    "    correct = total = n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for x_t, x_s, y in loader:\n",
    "            x_t = x_t.to(device, non_blocking=True)\n",
    "            x_s = x_s.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "\n",
    "            # Прогоны через модели вызовут хуки и сохранят токены\n",
    "            t_feat = teacher_model.encode_image(x_t)\n",
    "            t_logits = teacher_head(t_feat)  # не используется в лоссе, но может быть полезен\n",
    "            s_feat = student_model.encode_image(x_s)\n",
    "            s_logits = student_head(s_feat)\n",
    "\n",
    "            # Карты внимания из токенов\n",
    "            t_tokens = teacher_hook.outputs\n",
    "            s_tokens = student_hook.outputs\n",
    "            if isinstance(t_tokens, (tuple, list)): t_tokens = t_tokens[0]\n",
    "            if isinstance(s_tokens, (tuple, list)): s_tokens = s_tokens[0]\n",
    "            attn_t = tokens_to_attention_map(t_tokens).detach()\n",
    "            attn_s = tokens_to_attention_map(s_tokens)\n",
    "\n",
    "            attn_loss = match_attention_maps(attn_s, attn_t)\n",
    "            ce_loss = F.cross_entropy(s_logits, y)\n",
    "            loss = AT_ALPHA * attn_loss + (1.0 - AT_ALPHA) * ce_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_attn += attn_loss.item()\n",
    "            total_ce += ce_loss.item()\n",
    "            _, pred = torch.max(s_logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            n_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    avg_attn = total_attn / max(1, n_batches)\n",
    "    avg_ce = total_ce / max(1, n_batches)\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return avg_loss, avg_attn, avg_ce, acc\n",
    "\n",
    "print('✓ Функция валидации готова')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d28e3",
   "metadata": {},
   "source": [
    "# 5️⃣ TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d6b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "НАЧАЛО ОБУЧЕНИЯ С ATTENTION DISTILLATION (AT)\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/15 | Train: loss=0.5121 attn=0.0005 ce=1.2794 | Val: loss=0.3987 attn=0.0010 ce=0.9952 acc=52.66%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=52.66%)\n",
      "Epoch 2/15 | Train: loss=0.3576 attn=0.0005 ce=0.8932 | Val: loss=0.3200 attn=0.0003 ce=0.7994 acc=60.62%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=60.62%)\n",
      "Epoch 3/15 | Train: loss=0.3232 attn=0.0003 ce=0.8074 | Val: loss=0.3063 attn=0.0004 ce=0.7651 acc=61.25%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=61.25%)\n",
      "Epoch 4/15 | Train: loss=0.2865 attn=0.0004 ce=0.7155 | Val: loss=0.2936 attn=0.0004 ce=0.7335 acc=63.12%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=63.12%)\n",
      "Epoch 5/15 | Train: loss=0.2799 attn=0.0004 ce=0.6991 | Val: loss=0.2800 attn=0.0003 ce=0.6995 acc=69.22%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=69.22%)\n",
      "Epoch 6/15 | Train: loss=0.2416 attn=0.0004 ce=0.6035 | Val: loss=0.2531 attn=0.0004 ce=0.6322 acc=70.47%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=70.47%)\n",
      "Epoch 7/15 | Train: loss=0.2126 attn=0.0004 ce=0.5311 | Val: loss=0.2493 attn=0.0004 ce=0.6227 acc=75.47%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=75.47%)\n",
      "Epoch 8/15 | Train: loss=0.1784 attn=0.0004 ce=0.4455 | Val: loss=0.2384 attn=0.0004 ce=0.5955 acc=73.44%\n",
      "Epoch 9/15 | Train: loss=0.1361 attn=0.0004 ce=0.3395 | Val: loss=0.2280 attn=0.0004 ce=0.5695 acc=74.84%\n",
      "Epoch 10/15 | Train: loss=0.1088 attn=0.0004 ce=0.2713 | Val: loss=0.2646 attn=0.0004 ce=0.6610 acc=75.78%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=75.78%)\n",
      "Epoch 11/15 | Train: loss=0.0711 attn=0.0004 ce=0.1772 | Val: loss=0.2590 attn=0.0004 ce=0.6468 acc=78.75%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=78.75%)\n",
      "Epoch 12/15 | Train: loss=0.0396 attn=0.0004 ce=0.0984 | Val: loss=0.2568 attn=0.0004 ce=0.6414 acc=80.78%\n",
      "  ✓ Сохранена лучшая модель (ValAcc=80.78%)\n",
      "Epoch 13/15 | Train: loss=0.0254 attn=0.0004 ce=0.0629 | Val: loss=0.2986 attn=0.0004 ce=0.7459 acc=78.75%\n",
      "Epoch 14/15 | Train: loss=0.0139 attn=0.0004 ce=0.0343 | Val: loss=0.3011 attn=0.0004 ce=0.7522 acc=78.28%\n",
      "Epoch 15/15 | Train: loss=0.0095 attn=0.0004 ce=0.0233 | Val: loss=0.3052 attn=0.0004 ce=0.7625 acc=78.91%\n",
      "\n",
      "Завершено обучение AT. Лучшая ValAcc: 80.78%\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Обучение студента с Attention Distillation\n",
    "# ==============================\n",
    "optimizer = torch.optim.AdamW(list(student_model.parameters()) + list(student_head.parameters()), lr=AT_LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=AT_EPOCHS)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_attn': [], 'train_ce': [],\n",
    "    'val_loss': [], 'val_attn': [], 'val_ce': [], 'val_acc': []\n",
    "}\n",
    "best_val_acc, best_state = -1.0, None\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('НАЧАЛО ОБУЧЕНИЯ С ATTENTION DISTILLATION (AT)')\n",
    "print('='*80 + '\\n')\n",
    "\n",
    "for epoch in range(1, AT_EPOCHS + 1):\n",
    "    student_model.train(); student_head.train()\n",
    "    teacher_model.eval(); teacher_head.eval()\n",
    "    loss_sum = attn_sum = ce_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x_t, x_s, y in DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available()):\n",
    "        x_t = x_t.to(device, non_blocking=True)\n",
    "        x_s = x_s.to(device, non_blocking=True)\n",
    "        y   = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Учитель (фичи + токены via hook)\n",
    "        with torch.no_grad():\n",
    "            t_feat = teacher_model.encode_image(x_t)\n",
    "            _ = teacher_head(t_feat)  # чтобы быть консистентным с валидацией\n",
    "        t_tokens = teacher_hook.outputs\n",
    "        if isinstance(t_tokens, (tuple, list)): t_tokens = t_tokens[0]\n",
    "        attn_t = tokens_to_attention_map(t_tokens).detach()\n",
    "\n",
    "        # Студент\n",
    "        s_feat = student_model.encode_image(x_s)\n",
    "        s_logits = student_head(s_feat)\n",
    "        s_tokens = student_hook.outputs\n",
    "        if isinstance(s_tokens, (tuple, list)): s_tokens = s_tokens[0]\n",
    "        attn_s = tokens_to_attention_map(s_tokens)\n",
    "\n",
    "        loss, attn_loss, ce_loss = criterion(s_logits, y, attn_s, attn_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item(); attn_sum += attn_loss.item(); ce_sum += ce_loss.item(); n_batches += 1\n",
    "\n",
    "    avg_train_loss = loss_sum / max(1, n_batches)\n",
    "    avg_train_attn = attn_sum / max(1, n_batches)\n",
    "    avg_train_ce = ce_sum / max(1, n_batches)\n",
    "\n",
    "    val_loss, val_attn, val_ce, val_acc = validate(student_model, student_head, teacher_model, teacher_head, val_loader, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_attn'].append(avg_train_attn)\n",
    "    history['train_ce'].append(avg_train_ce)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_attn'].append(val_attn)\n",
    "    history['val_ce'].append(val_ce)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch}/{AT_EPOCHS} | Train: loss={avg_train_loss:.4f} attn={avg_train_attn:.4f} ce={avg_train_ce:.4f} | Val: loss={val_loss:.4f} attn={val_attn:.4f} ce={val_ce:.4f} acc={val_acc:.2f}%')\n",
    "\n",
    "    if SAVE_BEST and val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = {\n",
    "            'student_model': student_model.state_dict(),\n",
    "            'student_head': student_head.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_state, 'student_attn_best.pth')\n",
    "        print(f'  ✓ Сохранена лучшая модель (ValAcc={val_acc:.2f}%)')\n",
    "\n",
    "print('\\nЗавершено обучение AT. Лучшая ValAcc:', f'{best_val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb3712",
   "metadata": {},
   "source": [
    "# 6️⃣ EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930bf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.88%\n",
      "Accuracy по классам:\n",
      "  dyed-lifted-polyps: 81.00% (81/100)\n",
      "  dyed-resection-margins: 82.00% (82/100)\n",
      "  esophagitis: 67.00% (67/100)\n",
      "  normal-cecum: 93.00% (93/100)\n",
      "  normal-pylorus: 100.00% (100/100)\n",
      "  normal-z-line: 67.00% (67/100)\n",
      "  polyps: 84.00% (84/100)\n",
      "  ulcerative-colitis: 73.00% (73/100)\n",
      "✓ Финальная модель сохранена: student_attn_final.pth\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Тестирование и сохранение\n",
    "# ==============================\n",
    "def test_student(student_model, student_head, loader, device):\n",
    "    student_model.eval(); student_head.eval()\n",
    "    correct, total = 0, 0\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "    with torch.no_grad():\n",
    "        for _, x_s, y in loader:\n",
    "            x_s = x_s.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "            s_feat = student_model.encode_image(x_s)\n",
    "            logits = student_head(s_feat)\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            for i in range(y.size(0)):\n",
    "                cls_name = idx_to_label[y[i].item()]\n",
    "                class_total[cls_name] += 1\n",
    "                if pred[i] == y[i]:\n",
    "                    class_correct[cls_name] += 1\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return acc, class_correct, class_total\n",
    "\n",
    "test_acc, class_correct, class_total = test_student(student_model, student_head, test_loader, device)\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "print('Accuracy по классам:')\n",
    "for cls in sorted(class_total.keys()):\n",
    "    acc_c = 100.0 * class_correct[cls] / class_total[cls] if class_total[cls] > 0 else 0.0\n",
    "    print(f'  {cls}: {acc_c:.2f}% ({class_correct[cls]}/{class_total[cls]})')\n",
    "\n",
    "torch.save({\n",
    "    'student_model': student_model.state_dict(),\n",
    "    'student_head': student_head.state_dict(),\n",
    "    'classes': classes,\n",
    "    'label_to_idx': label_to_idx,\n",
    "    'idx_to_label': idx_to_label,\n",
    "    'teacher_dim': teacher_dim,\n",
    "    'student_dim': student_dim,\n",
    "    'num_classes': num_classes,\n",
    "    'history': history\n",
    "}, 'student_attn_final.pth')\n",
    "print('✓ Финальная модель сохранена: student_attn_final.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf0e94",
   "metadata": {},
   "source": [
    "# 7️⃣ RESULTS & VISUALIZATION & SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dc671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Визуализация метрик\n",
    "# ==============================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes[0,0].plot(history['train_loss'], label='Train'); axes[0,0].plot(history['val_loss'], label='Val'); axes[0,0].set_title('Total Loss'); axes[0,0].legend(); axes[0,0].grid(True)\n",
    "axes[0,1].plot(history['train_attn'], label='Train AT'); axes[0,1].plot(history['val_attn'], label='Val AT'); axes[0,1].set_title('Attention Loss'); axes[0,1].legend(); axes[0,1].grid(True)\n",
    "axes[1,0].plot(history['train_ce'], label='Train CE'); axes[1,0].plot(history['val_ce'], label='Val CE'); axes[1,0].set_title('CE Loss'); axes[1,0].legend(); axes[1,0].grid(True)\n",
    "axes[1,1].plot(history['val_acc'], label='Val Acc', color='green'); axes[1,1].set_title('Validation Accuracy'); axes[1,1].legend(); axes[1,1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_t_training_history.png', dpi=150, bbox_inches='tight')\n",
    "print('📈 История обучения сохранена: attention_t_training_history.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
