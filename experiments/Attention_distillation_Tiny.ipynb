{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e455471",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ea51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPU is available. Using CUDA\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, random, time\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('‚úì GPU is available. Using CUDA')\n",
    "else:\n",
    "    print('‚ö† Using CPU')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa76795",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ MODEL DOWNLOAD / SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3a4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# –ö–æ–Ω—Ñ–∏–≥\n",
    "ROOT_DIR = 'kvasir-dataset-v2'\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0  # Windows-safe\n",
    "\n",
    "# Teacher probe\n",
    "TEACHER_LR = 5e-4\n",
    "TEACHER_EPOCHS = 5\n",
    "\n",
    "# Attention distillation\n",
    "AT_LR = 1e-4\n",
    "AT_EPOCHS = 15\n",
    "AT_ALPHA = 0.6  # –≤–µ—Å attention MSE, (1-AT_ALPHA) ‚Äî CE\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "SAVE_BEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f256a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ViT_opti\\ViT_optimization\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12aae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ViT_opti\\ViT_optimization\\perception_models\n"
     ]
    }
   ],
   "source": [
    "cd perception_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe89c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ViT_opti\\ViT_optimization\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å-—É—á–∏—Ç–µ–ª—è PE-Core-L14-336...\n",
      "Missing keys for loading model: []\n",
      "Unexpected keys for loading model: []\n",
      "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å-—Å—Ç—É–¥–µ–Ω—Ç–∞ PE-Core-T16-384...\n",
      "Missing keys for loading model: []\n",
      "Unexpected keys for loading model: []\n",
      "–†–∞–∑–º–µ—Ä—ã: teacher_img=336, student_img=384\n",
      "Dims: teacher=1024, student=512\n"
     ]
    }
   ],
   "source": [
    "from core.vision_encoder import pe\n",
    "from core.vision_encoder import transforms\n",
    "\n",
    "print('–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å-—É—á–∏—Ç–µ–ª—è PE-Core-L14-336...')\n",
    "teacher_model = pe.CLIP.from_config('PE-Core-L14-336', pretrained=True).to(device).float().eval()\n",
    "\n",
    "print('–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å-—Å—Ç—É–¥–µ–Ω—Ç–∞ PE-Core-T16-384...')\n",
    "student_model = pe.CLIP.from_config('PE-Core-T16-384', pretrained=True).to(device).float()\n",
    "\n",
    "teacher_preprocessor = transforms.get_image_transform(teacher_model.image_size)\n",
    "student_preprocessor = transforms.get_image_transform(student_model.image_size)\n",
    "teacher_dim = teacher_model.visual.output_dim\n",
    "student_dim = student_model.visual.output_dim\n",
    "\n",
    "print(f'–†–∞–∑–º–µ—Ä—ã: teacher_img={teacher_model.image_size}, student_img={student_model.image_size}')\n",
    "print(f'Dims: teacher={teacher_dim}, student={student_dim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ee581",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8d7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ViT_opti\\ViT_optimization\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ecfb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirn\\OneDrive\\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\\ViT_opti\\ViT_optimization\\data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4b361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\n",
      "–ù–∞–π–¥–µ–Ω–æ 8000 jpg-—Ñ–∞–π–ª–æ–≤\n",
      "–í–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 7999\n",
      "–ö–ª–∞—Å—Å–æ–≤: 8 -> ['dyed-lifted-polyps', 'dyed-resection-margins', 'esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis']\n",
      "–ü–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ: 4000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "Train: 2560  Val: 640  Test: 800\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "# ==============================\n",
    "def extract_label_from_path(image_path, root_dir):\n",
    "    parts = Path(image_path).parts\n",
    "    try:\n",
    "        ridx = parts.index(Path(root_dir).name)\n",
    "        return parts[ridx + 1]\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "print('–ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...')\n",
    "all_image_paths = glob(os.path.join(ROOT_DIR, '**', '*.jpg'), recursive=True)\n",
    "print(f'–ù–∞–π–¥–µ–Ω–æ {len(all_image_paths)} jpg-—Ñ–∞–π–ª–æ–≤')\n",
    "\n",
    "image_to_path, image_to_label = {}, {}\n",
    "for p in all_image_paths:\n",
    "    image_id = os.path.splitext(os.path.basename(p))[0]\n",
    "    label = extract_label_from_path(p, ROOT_DIR)\n",
    "    if label is not None:\n",
    "        image_to_path[image_id] = p\n",
    "        image_to_label[image_id] = label\n",
    "print(f'–í–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_to_label)}')\n",
    "\n",
    "classes = sorted(list(set(image_to_label.values())))\n",
    "label_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "idx_to_label = {i: c for c, i in label_to_idx.items()}\n",
    "print(f'–ö–ª–∞—Å—Å–æ–≤: {len(classes)} -> {classes}')\n",
    "\n",
    "# –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ\n",
    "images_by_class = defaultdict(list)\n",
    "for img_id, lbl in image_to_label.items():\n",
    "    images_by_class[lbl].append(img_id)\n",
    "\n",
    "images_per_class = 500\n",
    "sampled = []\n",
    "for lbl, ids in images_by_class.items():\n",
    "    k = min(images_per_class, len(ids))\n",
    "    sampled.extend(random.sample(ids, k))\n",
    "random.shuffle(sampled)\n",
    "print(f'–ü–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ: {len(sampled)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π')\n",
    "\n",
    "labels_for_strat = [image_to_label[i] for i in sampled]\n",
    "train_val_ids, test_ids = train_test_split(sampled, test_size=0.20, stratify=labels_for_strat, random_state=SEED)\n",
    "train_labels_for_strat = [image_to_label[i] for i in train_val_ids]\n",
    "train_ids, val_ids = train_test_split(train_val_ids, test_size=0.20, stratify=train_labels_for_strat, random_state=SEED)\n",
    "\n",
    "print(f'Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88703578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DataLoaders –≥–æ—Ç–æ–≤—ã\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Dataset –∏ DataLoader\n",
    "# ==============================\n",
    "class AttnDataset(Dataset):\n",
    "    def __init__(self, image_ids, image_to_path, image_to_label, label_to_idx, t_transform, s_transform):\n",
    "        self.ids = image_ids\n",
    "        self.image_to_path = image_to_path\n",
    "        self.image_to_label = image_to_label\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.t_transform = t_transform\n",
    "        self.s_transform = s_transform\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        path = self.image_to_path[img_id]\n",
    "        label = self.image_to_label[img_id]\n",
    "        y = self.label_to_idx[label]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        x_t = self.t_transform(img)\n",
    "        x_s = self.s_transform(img)\n",
    "        return x_t, x_s, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_ds = AttnDataset(train_ids, image_to_path, image_to_label, label_to_idx, teacher_preprocessor, student_preprocessor)\n",
    "val_ds   = AttnDataset(val_ids,   image_to_path, image_to_label, label_to_idx, teacher_preprocessor, student_preprocessor)\n",
    "test_ds  = AttnDataset(test_ids,  image_to_path, image_to_label, label_to_idx, teacher_preprocessor, student_preprocessor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "print('‚úì DataLoaders –≥–æ—Ç–æ–≤—ã')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34577734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≥–æ–ª–æ–≤—ã: teacher_head 1024->8, student_head 512->8\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –ì–æ–ª–æ–≤—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "# ==============================\n",
    "class LinearHead(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "num_classes = len(classes)\n",
    "teacher_head = LinearHead(teacher_dim, num_classes).to(device)\n",
    "student_head = LinearHead(student_dim, num_classes).to(device)\n",
    "print(f'–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≥–æ–ª–æ–≤—ã: teacher_head {teacher_dim}->{num_classes}, student_head {student_dim}->{num_classes}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743dff9",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ MODEL PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher Probe] Epoch 1/5 | TrainAcc 68.91% | ValAcc 82.19% | ValLoss 0.6337\n",
      "[Teacher Probe] Epoch 2/5 | TrainAcc 85.43% | ValAcc 88.28% | ValLoss 0.4447\n",
      "[Teacher Probe] Epoch 3/5 | TrainAcc 87.30% | ValAcc 87.66% | ValLoss 0.3766\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –û–±—É—á–µ–Ω–∏–µ teacher linear probe (–∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–π —É—á–∏—Ç–µ–ª—å)\n",
    "# ==============================\n",
    "def evaluate_head(model, head, loader, device):\n",
    "    model.eval(); head.eval()\n",
    "    correct, total, loss_sum, n_batches = 0, 0, 0.0, 0\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x_t, _, y in loader:\n",
    "            x_t = x_t.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "            feats = model.encode_image(x_t)\n",
    "            logits = head(feats)\n",
    "            loss = ce(logits, y)\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            loss_sum += loss.item()\n",
    "            n_batches += 1\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return (loss_sum / max(1, n_batches)), acc\n",
    "\n",
    "def train_teacher_probe(model, head, train_loader, val_loader, epochs=TEACHER_EPOCHS, lr=TEACHER_LR):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    opt = torch.optim.AdamW(head.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    best_val_acc, best_state = -1.0, None\n",
    "    for ep in range(1, epochs + 1):\n",
    "        head.train()\n",
    "        loss_sum, n_batches, correct, total = 0.0, 0, 0, 0\n",
    "        for x_t, _, y in train_loader:\n",
    "            x_t = x_t.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "            with torch.no_grad():\n",
    "                feats = model.encode_image(x_t)\n",
    "            logits = head(feats)\n",
    "            loss = ce(logits, y)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            loss_sum += loss.item(); n_batches += 1\n",
    "        train_acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "        val_loss, val_acc = evaluate_head(model, head, val_loader, device)\n",
    "        print(f'[Teacher Probe] Epoch {ep}/{epochs} | TrainAcc {train_acc:.2f}% | ValAcc {val_acc:.2f}% | ValLoss {val_loss:.4f}')\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = head.state_dict()\n",
    "    if best_state is not None:\n",
    "        head.load_state_dict(best_state)\n",
    "    for p in head.parameters():\n",
    "        p.requires_grad = False\n",
    "    head.eval()\n",
    "    torch.save({'state_dict': head.state_dict(), 'num_classes': num_classes}, 'teacher_linear_probe_attn.pth')\n",
    "    print(f'‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω teacher linear probe (ValAcc={best_val_acc:.2f}%)')\n",
    "\n",
    "train_teacher_probe(teacher_model, teacher_head, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã forward-—Ö—É–∫–∏ –Ω–∞ visual.transformer (teacher/student)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –•—É–∫–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (ViT)\n",
    "# ==============================\n",
    "class ModuleOutputHook:\n",
    "    def __init__(self, module):\n",
    "        self.outputs = None\n",
    "        self.handle = module.register_forward_hook(self._hook)\n",
    "    def _hook(self, module, inputs, output):\n",
    "        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã—Ö–æ–¥ (–æ–±—ã—á–Ω–æ [B, N, D] –¥–ª—è transformer)\n",
    "        self.outputs = output\n",
    "    def close(self):\n",
    "        if self.handle is not None:\n",
    "            self.handle.remove(); self.handle = None\n",
    "\n",
    "def get_transformer_module(clip_model):\n",
    "    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥—É–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è ViT\n",
    "    visual = getattr(clip_model, 'visual', None)\n",
    "    assert visual is not None, 'visual –Ω–µ –Ω–∞–π–¥–µ–Ω —É CLIP-–º–æ–¥–µ–ª–∏'\n",
    "    transformer = getattr(visual, 'transformer', None)\n",
    "    assert transformer is not None, 'visual.transformer –Ω–µ –Ω–∞–π–¥–µ–Ω (–æ–∂–∏–¥–∞–µ—Ç—Å—è ViT)'\n",
    "    return transformer\n",
    "\n",
    "teacher_transformer = get_transformer_module(teacher_model)\n",
    "student_transformer = get_transformer_module(student_model)\n",
    "teacher_hook = ModuleOutputHook(teacher_transformer)\n",
    "student_hook = ModuleOutputHook(student_transformer)\n",
    "print('‚úì –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã forward-—Ö—É–∫–∏ –Ω–∞ visual.transformer (teacher/student)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b62eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì –§—É–Ω–∫—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "# ==============================\n",
    "def tokens_to_attention_map(tokens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    tokens: [B, N, D] –∏–ª–∏ [B, N+1, D] (–µ—Å–ª–∏ –µ—Å—Ç—å CLS —Ç–æ–∫–µ–Ω).\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–∞—Ä—Ç—É –≤–Ω–∏–º–∞–Ω–∏—è [B, H, W] –∫–∞–∫ —Å—É–º–º—É –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –ø–æ –∫–∞–Ω–∞–ª–∞–º.\n",
    "    \"\"\"\n",
    "    assert tokens.dim() == 3, '–û–∂–∏–¥–∞–µ—Ç—Å—è —Ç–µ–Ω–∑–æ—Ä —Å —Ñ–æ—Ä–º–æ–π [B, N, D]'\n",
    "    B, N, D = tokens.shape\n",
    "    s = int(math.sqrt(N))\n",
    "    if s * s == N:\n",
    "        patch_tokens = tokens  # CLS –Ω–µ—Ç\n",
    "        H = W = s\n",
    "    else:\n",
    "        # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω ‚Äî CLS, –∑–Ω–∞—á–∏—Ç –ø–∞—Ç—á–µ–π N-1 = H*W\n",
    "        Np = N - 1\n",
    "        H = W = int(math.sqrt(Np))\n",
    "        assert H * W == Np, '–ß–∏—Å–ª–æ –ø–∞—Ç—á-—Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –∫–≤–∞–¥—Ä–∞—Ç: –Ω–µ —É–¥–∞–µ—Ç—Å—è –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–∞—Ä—Ç—É'\n",
    "        patch_tokens = tokens[:, 1:, :]\n",
    "    fmap = patch_tokens.view(B, H, W, D)\n",
    "    attn = (fmap.pow(2).sum(dim=-1))  # [B, H, W]\n",
    "    # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞—Ä—Ç–µ, —á—Ç–æ–±—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—ã–ª–æ –º–∞—Å—à—Ç–∞–±-–∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–º\n",
    "    attn = attn / (attn.norm(p=2, dim=(1, 2), keepdim=True) + 1e-6)\n",
    "    return attn\n",
    "\n",
    "def match_attention_maps(attn_s: torch.Tensor, attn_t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    –ü—Ä–∏–≤–æ–¥–∏—Ç –∫–∞—Ä—Ç—ã –∫ –æ–¥–Ω–æ–º—É —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é –∏ —Å—á–∏—Ç–∞–µ—Ç MSE.\n",
    "    attn_*: [B, H, W]\n",
    "    \"\"\"\n",
    "    if attn_s.shape != attn_t.shape:\n",
    "        attn_t_ = F.interpolate(attn_t.unsqueeze(1), size=attn_s.shape[-2:], mode='bilinear', align_corners=False).squeeze(1)\n",
    "    else:\n",
    "        attn_t_ = attn_t\n",
    "    return F.mse_loss(attn_s, attn_t_)\n",
    "\n",
    "print('‚úì –§—É–Ω–∫—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3478aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫—Ä–∏—Ç–µ—Ä–∏–π AttentionDistillLoss\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –ö—Ä–∏—Ç–µ—Ä–∏–π: Attention MSE + CE\n",
    "# ==============================\n",
    "class AttentionDistillLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.6):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "    def forward(self, s_logits, y, attn_s, attn_t):\n",
    "        ce_loss = self.ce(s_logits, y)\n",
    "        attn_loss = match_attention_maps(attn_s, attn_t)\n",
    "        total = self.alpha * attn_loss + (1.0 - self.alpha) * ce_loss\n",
    "        return total, attn_loss, ce_loss\n",
    "\n",
    "criterion = AttentionDistillLoss(alpha=AT_ALPHA)\n",
    "print('‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫—Ä–∏—Ç–µ—Ä–∏–π AttentionDistillLoss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b95308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì –§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≥–æ—Ç–æ–≤–∞\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "# ==============================\n",
    "def validate(student_model, student_head, teacher_model, teacher_head, loader, device):\n",
    "    student_model.eval(); student_head.eval()\n",
    "    teacher_model.eval(); teacher_head.eval()\n",
    "    total_loss = total_attn = total_ce = 0.0\n",
    "    correct = total = n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for x_t, x_s, y in loader:\n",
    "            x_t = x_t.to(device, non_blocking=True)\n",
    "            x_s = x_s.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "\n",
    "            # –ü—Ä–æ–≥–æ–Ω—ã —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª–∏ –≤—ã–∑–æ–≤—É—Ç —Ö—É–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç —Ç–æ–∫–µ–Ω—ã\n",
    "            t_feat = teacher_model.encode_image(x_t)\n",
    "            t_logits = teacher_head(t_feat)  # –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ª–æ—Å—Å–µ, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω\n",
    "            s_feat = student_model.encode_image(x_s)\n",
    "            s_logits = student_head(s_feat)\n",
    "\n",
    "            # –ö–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            t_tokens = teacher_hook.outputs\n",
    "            s_tokens = student_hook.outputs\n",
    "            if isinstance(t_tokens, (tuple, list)): t_tokens = t_tokens[0]\n",
    "            if isinstance(s_tokens, (tuple, list)): s_tokens = s_tokens[0]\n",
    "            attn_t = tokens_to_attention_map(t_tokens).detach()\n",
    "            attn_s = tokens_to_attention_map(s_tokens)\n",
    "\n",
    "            attn_loss = match_attention_maps(attn_s, attn_t)\n",
    "            ce_loss = F.cross_entropy(s_logits, y)\n",
    "            loss = AT_ALPHA * attn_loss + (1.0 - AT_ALPHA) * ce_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_attn += attn_loss.item()\n",
    "            total_ce += ce_loss.item()\n",
    "            _, pred = torch.max(s_logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            n_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    avg_attn = total_attn / max(1, n_batches)\n",
    "    avg_ce = total_ce / max(1, n_batches)\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return avg_loss, avg_attn, avg_ce, acc\n",
    "\n",
    "print('‚úì –§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≥–æ—Ç–æ–≤–∞')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d28e3",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d6b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –° ATTENTION DISTILLATION (AT)\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/15 | Train: loss=0.5121 attn=0.0005 ce=1.2794 | Val: loss=0.3987 attn=0.0010 ce=0.9952 acc=52.66%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=52.66%)\n",
      "Epoch 2/15 | Train: loss=0.3576 attn=0.0005 ce=0.8932 | Val: loss=0.3200 attn=0.0003 ce=0.7994 acc=60.62%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=60.62%)\n",
      "Epoch 3/15 | Train: loss=0.3232 attn=0.0003 ce=0.8074 | Val: loss=0.3063 attn=0.0004 ce=0.7651 acc=61.25%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=61.25%)\n",
      "Epoch 4/15 | Train: loss=0.2865 attn=0.0004 ce=0.7155 | Val: loss=0.2936 attn=0.0004 ce=0.7335 acc=63.12%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=63.12%)\n",
      "Epoch 5/15 | Train: loss=0.2799 attn=0.0004 ce=0.6991 | Val: loss=0.2800 attn=0.0003 ce=0.6995 acc=69.22%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=69.22%)\n",
      "Epoch 6/15 | Train: loss=0.2416 attn=0.0004 ce=0.6035 | Val: loss=0.2531 attn=0.0004 ce=0.6322 acc=70.47%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=70.47%)\n",
      "Epoch 7/15 | Train: loss=0.2126 attn=0.0004 ce=0.5311 | Val: loss=0.2493 attn=0.0004 ce=0.6227 acc=75.47%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=75.47%)\n",
      "Epoch 8/15 | Train: loss=0.1784 attn=0.0004 ce=0.4455 | Val: loss=0.2384 attn=0.0004 ce=0.5955 acc=73.44%\n",
      "Epoch 9/15 | Train: loss=0.1361 attn=0.0004 ce=0.3395 | Val: loss=0.2280 attn=0.0004 ce=0.5695 acc=74.84%\n",
      "Epoch 10/15 | Train: loss=0.1088 attn=0.0004 ce=0.2713 | Val: loss=0.2646 attn=0.0004 ce=0.6610 acc=75.78%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=75.78%)\n",
      "Epoch 11/15 | Train: loss=0.0711 attn=0.0004 ce=0.1772 | Val: loss=0.2590 attn=0.0004 ce=0.6468 acc=78.75%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=78.75%)\n",
      "Epoch 12/15 | Train: loss=0.0396 attn=0.0004 ce=0.0984 | Val: loss=0.2568 attn=0.0004 ce=0.6414 acc=80.78%\n",
      "  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc=80.78%)\n",
      "Epoch 13/15 | Train: loss=0.0254 attn=0.0004 ce=0.0629 | Val: loss=0.2986 attn=0.0004 ce=0.7459 acc=78.75%\n",
      "Epoch 14/15 | Train: loss=0.0139 attn=0.0004 ce=0.0343 | Val: loss=0.3011 attn=0.0004 ce=0.7522 acc=78.28%\n",
      "Epoch 15/15 | Train: loss=0.0095 attn=0.0004 ce=0.0233 | Val: loss=0.3052 attn=0.0004 ce=0.7625 acc=78.91%\n",
      "\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ AT. –õ—É—á—à–∞—è ValAcc: 80.78%\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –û–±—É—á–µ–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å Attention Distillation\n",
    "# ==============================\n",
    "optimizer = torch.optim.AdamW(list(student_model.parameters()) + list(student_head.parameters()), lr=AT_LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=AT_EPOCHS)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_attn': [], 'train_ce': [],\n",
    "    'val_loss': [], 'val_attn': [], 'val_ce': [], 'val_acc': []\n",
    "}\n",
    "best_val_acc, best_state = -1.0, None\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –° ATTENTION DISTILLATION (AT)')\n",
    "print('='*80 + '\\n')\n",
    "\n",
    "for epoch in range(1, AT_EPOCHS + 1):\n",
    "    student_model.train(); student_head.train()\n",
    "    teacher_model.eval(); teacher_head.eval()\n",
    "    loss_sum = attn_sum = ce_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x_t, x_s, y in DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available()):\n",
    "        x_t = x_t.to(device, non_blocking=True)\n",
    "        x_s = x_s.to(device, non_blocking=True)\n",
    "        y   = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # –£—á–∏—Ç–µ–ª—å (—Ñ–∏—á–∏ + —Ç–æ–∫–µ–Ω—ã via hook)\n",
    "        with torch.no_grad():\n",
    "            t_feat = teacher_model.encode_image(x_t)\n",
    "            _ = teacher_head(t_feat)  # —á—Ç–æ–±—ã –±—ã—Ç—å –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–º —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\n",
    "        t_tokens = teacher_hook.outputs\n",
    "        if isinstance(t_tokens, (tuple, list)): t_tokens = t_tokens[0]\n",
    "        attn_t = tokens_to_attention_map(t_tokens).detach()\n",
    "\n",
    "        # –°—Ç—É–¥–µ–Ω—Ç\n",
    "        s_feat = student_model.encode_image(x_s)\n",
    "        s_logits = student_head(s_feat)\n",
    "        s_tokens = student_hook.outputs\n",
    "        if isinstance(s_tokens, (tuple, list)): s_tokens = s_tokens[0]\n",
    "        attn_s = tokens_to_attention_map(s_tokens)\n",
    "\n",
    "        loss, attn_loss, ce_loss = criterion(s_logits, y, attn_s, attn_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item(); attn_sum += attn_loss.item(); ce_sum += ce_loss.item(); n_batches += 1\n",
    "\n",
    "    avg_train_loss = loss_sum / max(1, n_batches)\n",
    "    avg_train_attn = attn_sum / max(1, n_batches)\n",
    "    avg_train_ce = ce_sum / max(1, n_batches)\n",
    "\n",
    "    val_loss, val_attn, val_ce, val_acc = validate(student_model, student_head, teacher_model, teacher_head, val_loader, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_attn'].append(avg_train_attn)\n",
    "    history['train_ce'].append(avg_train_ce)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_attn'].append(val_attn)\n",
    "    history['val_ce'].append(val_ce)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch}/{AT_EPOCHS} | Train: loss={avg_train_loss:.4f} attn={avg_train_attn:.4f} ce={avg_train_ce:.4f} | Val: loss={val_loss:.4f} attn={val_attn:.4f} ce={val_ce:.4f} acc={val_acc:.2f}%')\n",
    "\n",
    "    if SAVE_BEST and val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = {\n",
    "            'student_model': student_model.state_dict(),\n",
    "            'student_head': student_head.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_state, 'student_attn_best.pth')\n",
    "        print(f'  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (ValAcc={val_acc:.2f}%)')\n",
    "\n",
    "print('\\n–ó–∞–≤–µ—Ä—à–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ AT. –õ—É—á—à–∞—è ValAcc:', f'{best_val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb3712",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930bf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.88%\n",
      "Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º:\n",
      "  dyed-lifted-polyps: 81.00% (81/100)\n",
      "  dyed-resection-margins: 82.00% (82/100)\n",
      "  esophagitis: 67.00% (67/100)\n",
      "  normal-cecum: 93.00% (93/100)\n",
      "  normal-pylorus: 100.00% (100/100)\n",
      "  normal-z-line: 67.00% (67/100)\n",
      "  polyps: 84.00% (84/100)\n",
      "  ulcerative-colitis: 73.00% (73/100)\n",
      "‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: student_attn_final.pth\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "# ==============================\n",
    "def test_student(student_model, student_head, loader, device):\n",
    "    student_model.eval(); student_head.eval()\n",
    "    correct, total = 0, 0\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "    with torch.no_grad():\n",
    "        for _, x_s, y in loader:\n",
    "            x_s = x_s.to(device, non_blocking=True)\n",
    "            y   = y.to(device, non_blocking=True)\n",
    "            s_feat = student_model.encode_image(x_s)\n",
    "            logits = student_head(s_feat)\n",
    "            _, pred = torch.max(logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            for i in range(y.size(0)):\n",
    "                cls_name = idx_to_label[y[i].item()]\n",
    "                class_total[cls_name] += 1\n",
    "                if pred[i] == y[i]:\n",
    "                    class_correct[cls_name] += 1\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    return acc, class_correct, class_total\n",
    "\n",
    "test_acc, class_correct, class_total = test_student(student_model, student_head, test_loader, device)\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "print('Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º:')\n",
    "for cls in sorted(class_total.keys()):\n",
    "    acc_c = 100.0 * class_correct[cls] / class_total[cls] if class_total[cls] > 0 else 0.0\n",
    "    print(f'  {cls}: {acc_c:.2f}% ({class_correct[cls]}/{class_total[cls]})')\n",
    "\n",
    "torch.save({\n",
    "    'student_model': student_model.state_dict(),\n",
    "    'student_head': student_head.state_dict(),\n",
    "    'classes': classes,\n",
    "    'label_to_idx': label_to_idx,\n",
    "    'idx_to_label': idx_to_label,\n",
    "    'teacher_dim': teacher_dim,\n",
    "    'student_dim': student_dim,\n",
    "    'num_classes': num_classes,\n",
    "    'history': history\n",
    "}, 'student_attn_final.pth')\n",
    "print('‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: student_attn_final.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf0e94",
   "metadata": {},
   "source": [
    "# 7Ô∏è‚É£ RESULTS & VISUALIZATION & SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dc671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "# ==============================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes[0,0].plot(history['train_loss'], label='Train'); axes[0,0].plot(history['val_loss'], label='Val'); axes[0,0].set_title('Total Loss'); axes[0,0].legend(); axes[0,0].grid(True)\n",
    "axes[0,1].plot(history['train_attn'], label='Train AT'); axes[0,1].plot(history['val_attn'], label='Val AT'); axes[0,1].set_title('Attention Loss'); axes[0,1].legend(); axes[0,1].grid(True)\n",
    "axes[1,0].plot(history['train_ce'], label='Train CE'); axes[1,0].plot(history['val_ce'], label='Val CE'); axes[1,0].set_title('CE Loss'); axes[1,0].legend(); axes[1,0].grid(True)\n",
    "axes[1,1].plot(history['val_acc'], label='Val Acc', color='green'); axes[1,1].set_title('Validation Accuracy'); axes[1,1].legend(); axes[1,1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_t_training_history.png', dpi=150, bbox_inches='tight')\n",
    "print('üìà –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: attention_t_training_history.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
