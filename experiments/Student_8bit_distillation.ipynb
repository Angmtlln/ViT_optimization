{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T09:25:48.740062Z",
     "start_time": "2025-07-30T09:25:44.287776Z"
    }
   },
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1) Модели\n",
    "teacher = timm.create_model(\"deit_small_patch16_224\", num_classes=10, pretrained=False)\n",
    "ckpt = torch.load(\"../data/model_weights/deit_small_cifar10.pth\", map_location='cpu')\n",
    "teacher.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "teacher.eval().to(device)\n",
    "\n",
    "student = timm.create_model(\"deit_tiny_patch16_224\", num_classes=10, pretrained=False)\n",
    "student.train().to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir_ubuntu/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:37:07.694903Z",
     "start_time": "2025-07-30T09:37:07.575783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ffcv.fields.rgb_image import RandomResizedCropRGBImageDecoder\n",
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.fields.decoders import SimpleRGBImageDecoder, NDArrayDecoder\n",
    "from ffcv.transforms import (\n",
    "    RandomResizedCrop, RandomHorizontalFlip,\n",
    "    ToTensor, ToTorchImage, Convert, ToDevice,Cutout\n",
    ")\n",
    "\n",
    "# 2) Даталоадер из .beton\n",
    "data_path = './data/cifar10_cutmix.beton'\n",
    "assert os.path.exists(data_path), f\"{data_path} not found\"\n",
    "\n",
    "# 1) Image pipeline: только FFCV‑де­кодер + FFCV‑трансформы\n",
    "image_pipeline = [\n",
    "    RandomResizedCropRGBImageDecoder((224, 224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    Cutout(16),                # нарезает «дырки» — эквивалент augment\n",
    "    ToTensor(),              # делает torch.tensor и нормализует [0,255]→[0,255]\n",
    "    ToTorchImage(),          # переставляет оси [H,W,C]→[C,H,W]\n",
    "    ToDevice(device, non_blocking=True),\n",
    "]\n",
    "\n",
    "# 2) Label pipeline: soft‑labels\n",
    "label_pipeline = [\n",
    "    NDArrayDecoder(),        # читает float32[10] soft‑labels\n",
    "    ToTensor(),              # делает torch.tensor\n",
    "    ToDevice(device, non_blocking=True),\n",
    "]\n",
    "\n",
    "train_loader = Loader(\n",
    "    data_path,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    order=OrderOption.RANDOM,\n",
    "    drop_last=True,\n",
    "    pipelines={\n",
    "        'image': image_pipeline,\n",
    "        'label': label_pipeline\n",
    "    },\n",
    ")"
   ],
   "id": "3210efd2c8529d4f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:37:09.088610Z",
     "start_time": "2025-07-30T09:37:09.085881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3) Loss с учётом soft-лейблов и teacher logits\n",
    "def distillation_loss(student_logits, teacher_logits, soft_labels, alpha=0.5, T=2.0):\n",
    "    # KL между student и teacher (температурная дистилляция)\n",
    "    kl_st = F.kl_div(\n",
    "        F.log_softmax(student_logits / T, dim=1),\n",
    "        F.softmax(teacher_logits / T, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (T * T)\n",
    "\n",
    "    # Soft‑CE между student и CutMix‑метками\n",
    "    ce_soft = F.kl_div(\n",
    "        F.log_softmax(student_logits, dim=1),\n",
    "        soft_labels,\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "\n",
    "    return alpha * kl_st + (1. - alpha) * ce_soft\n"
   ],
   "id": "6bfd05de01872168",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:38:28.296459Z",
     "start_time": "2025-07-30T09:38:27.975704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4) Оптимизатор и тренировочный цикл\n",
    "optimizer = Adam(student.parameters(), lr=3e-4)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        imgs = batch[0]\n",
    "        soft_labels = batch[1]\n",
    "        #imgs = batch['image']\n",
    "        #soft_labels = batch['label']    # float32 shape [B,10]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_logits = teacher(imgs)\n",
    "\n",
    "        s_logits = student(imgs)\n",
    "        loss = distillation_loss(s_logits, t_logits, soft_labels, alpha=0.7, T=2.0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch:2d} — avg loss: {avg:.4f}\")"
   ],
   "id": "f86f8024f77b77f3",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m#imgs = batch['image']\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m#soft_labels = batch['label']    # float32 shape [B,10]\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     t_logits = \u001B[43mteacher\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m s_logits = student(imgs)\n\u001B[32m     16\u001B[39m loss = distillation_loss(s_logits, t_logits, soft_labels, alpha=\u001B[32m0.7\u001B[39m, T=\u001B[32m2.0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/timm/models/vision_transformer.py:993\u001B[39m, in \u001B[36mVisionTransformer.forward\u001B[39m\u001B[34m(self, x, attn_mask)\u001B[39m\n\u001B[32m    992\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m) -> torch.Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m993\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    994\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.forward_head(x)\n\u001B[32m    995\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/timm/models/vision_transformer.py:936\u001B[39m, in \u001B[36mVisionTransformer.forward_features\u001B[39m\u001B[34m(self, x, attn_mask)\u001B[39m\n\u001B[32m    934\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward_features\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m) -> torch.Tensor:\n\u001B[32m    935\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Forward pass through feature layers (embeddings, transformer blocks, post-transformer norm).\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m936\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpatch_embed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    937\u001B[39m     x = \u001B[38;5;28mself\u001B[39m._pos_embed(x)\n\u001B[32m    938\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.patch_drop(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/timm/layers/patch_embed.py:131\u001B[39m, in \u001B[36mPatchEmbed.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    129\u001B[39m     pad_w = (\u001B[38;5;28mself\u001B[39m.patch_size[\u001B[32m1\u001B[39m] - W % \u001B[38;5;28mself\u001B[39m.patch_size[\u001B[32m1\u001B[39m]) % \u001B[38;5;28mself\u001B[39m.patch_size[\u001B[32m1\u001B[39m]\n\u001B[32m    130\u001B[39m     x = F.pad(x, (\u001B[32m0\u001B[39m, pad_w, \u001B[32m0\u001B[39m, pad_h))\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mproj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.flatten:\n\u001B[32m    133\u001B[39m     x = x.flatten(\u001B[32m2\u001B[39m).transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)  \u001B[38;5;66;03m# NCHW -> NLC\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/transformer-model-optimization/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cf5782fdc6d5126a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
